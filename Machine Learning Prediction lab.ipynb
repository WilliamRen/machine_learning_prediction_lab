{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<center>\n    <img src=\"https://gitlab.com/ibm/skills-network/courses/placeholder101/-/raw/master/labs/module%201/images/IDSNlogo.png\" width=\"300\" alt=\"cognitiveclass.ai logo\"  />\n</center>\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# **Space X  Falcon 9 First Stage Landing Prediction**\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Assignment:  Machine Learning Prediction\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Estimated time needed: **60** minutes\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Space X advertises Falcon 9 rocket launches on its website with a cost of 62 million dollars; other providers cost upward of 165 million dollars each, much of the savings is because Space X can reuse the first stage. Therefore if we can determine if the first stage will land, we can determine the cost of a launch. This information can be used if an alternate company wants to bid against space X for a rocket launch.   In this lab, you will create a machine learning pipeline  to predict if the first stage will land given the data from the preceding labs.\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DS0701EN-SkillsNetwork/api/Images/landing\\_1.gif)\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Several examples of an unsuccessful landing are shown here:\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DS0701EN-SkillsNetwork/api/Images/crash.gif)\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Most unsuccessful landings are planed. Space X; performs a controlled landing in the oceans.\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Objectives\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Perform exploratory  Data Analysis and determine Training Labels\n\n*   create a column for the class\n*   Standardize the data\n*   Split into training data and test data\n\n\\-Find best Hyperparameter for SVM, Classification Trees and Logistic Regression\n\n*   Find the method performs best using test data\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": ""
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "***\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Import Libraries and Define Auxiliary Functions\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We will import the following libraries for the lab\n"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": "# Pandas is a software library written for the Python programming language for data manipulation and analysis.\nimport pandas as pd\n# NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays\nimport numpy as np\n# Matplotlib is a plotting library for python and pyplot gives us a MatLab like plotting framework. We will use this in our plotter function to plot data.\nimport matplotlib.pyplot as plt\n#Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics\nimport seaborn as sns\n# Preprocessing allows us to standarsize our data\nfrom sklearn import preprocessing\n# Allows us to split our data into training and testing data\nfrom sklearn.model_selection import train_test_split\n# Allows us to test parameters of classification algorithms and find the best one\nfrom sklearn.model_selection import GridSearchCV\n# Logistic Regression classification algorithm\nfrom sklearn.linear_model import LogisticRegression\n# Support Vector Machine classification algorithm\nfrom sklearn.svm import SVC\n# Decision Tree classification algorithm\nfrom sklearn.tree import DecisionTreeClassifier\n# K Nearest Neighbors classification algorithm\nfrom sklearn.neighbors import KNeighborsClassifier"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "This function is to plot the confusion matrix.\n"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": "def plot_confusion_matrix(y,y_predict):\n    \"this function plots the confusion matrix\"\n    from sklearn.metrics import confusion_matrix\n\n    cm = confusion_matrix(y, y_predict)\n    ax= plt.subplot()\n    sns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells\n    ax.set_xlabel('Predicted labels')\n    ax.set_ylabel('True labels')\n    ax.set_title('Confusion Matrix'); \n    ax.xaxis.set_ticklabels(['did not land', 'land']); ax.yaxis.set_ticklabels(['did not land', 'landed'])"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Load the dataframe\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Load the data\n"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>FlightNumber</th>\n      <th>Date</th>\n      <th>BoosterVersion</th>\n      <th>PayloadMass</th>\n      <th>Orbit</th>\n      <th>LaunchSite</th>\n      <th>Outcome</th>\n      <th>Flights</th>\n      <th>GridFins</th>\n      <th>Reused</th>\n      <th>Legs</th>\n      <th>LandingPad</th>\n      <th>Block</th>\n      <th>ReusedCount</th>\n      <th>Serial</th>\n      <th>Longitude</th>\n      <th>Latitude</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2010-06-04</td>\n      <td>Falcon 9</td>\n      <td>6104.959412</td>\n      <td>LEO</td>\n      <td>CCAFS SLC 40</td>\n      <td>None None</td>\n      <td>1</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>B0003</td>\n      <td>-80.577366</td>\n      <td>28.561857</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>2012-05-22</td>\n      <td>Falcon 9</td>\n      <td>525.000000</td>\n      <td>LEO</td>\n      <td>CCAFS SLC 40</td>\n      <td>None None</td>\n      <td>1</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>B0005</td>\n      <td>-80.577366</td>\n      <td>28.561857</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>2013-03-01</td>\n      <td>Falcon 9</td>\n      <td>677.000000</td>\n      <td>ISS</td>\n      <td>CCAFS SLC 40</td>\n      <td>None None</td>\n      <td>1</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>B0007</td>\n      <td>-80.577366</td>\n      <td>28.561857</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>2013-09-29</td>\n      <td>Falcon 9</td>\n      <td>500.000000</td>\n      <td>PO</td>\n      <td>VAFB SLC 4E</td>\n      <td>False Ocean</td>\n      <td>1</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>B1003</td>\n      <td>-120.610829</td>\n      <td>34.632093</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>2013-12-03</td>\n      <td>Falcon 9</td>\n      <td>3170.000000</td>\n      <td>GTO</td>\n      <td>CCAFS SLC 40</td>\n      <td>None None</td>\n      <td>1</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>B1004</td>\n      <td>-80.577366</td>\n      <td>28.561857</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "   FlightNumber        Date BoosterVersion  PayloadMass Orbit    LaunchSite  \\\n0             1  2010-06-04       Falcon 9  6104.959412   LEO  CCAFS SLC 40   \n1             2  2012-05-22       Falcon 9   525.000000   LEO  CCAFS SLC 40   \n2             3  2013-03-01       Falcon 9   677.000000   ISS  CCAFS SLC 40   \n3             4  2013-09-29       Falcon 9   500.000000    PO   VAFB SLC 4E   \n4             5  2013-12-03       Falcon 9  3170.000000   GTO  CCAFS SLC 40   \n\n       Outcome  Flights  GridFins  Reused   Legs LandingPad  Block  \\\n0    None None        1     False   False  False        NaN    1.0   \n1    None None        1     False   False  False        NaN    1.0   \n2    None None        1     False   False  False        NaN    1.0   \n3  False Ocean        1     False   False  False        NaN    1.0   \n4    None None        1     False   False  False        NaN    1.0   \n\n   ReusedCount Serial   Longitude   Latitude  Class  \n0            0  B0003  -80.577366  28.561857      0  \n1            0  B0005  -80.577366  28.561857      0  \n2            0  B0007  -80.577366  28.561857      0  \n3            0  B1003 -120.610829  34.632093      0  \n4            0  B1004  -80.577366  28.561857      0  "
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "data = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DS0321EN-SkillsNetwork/datasets/dataset_part_2.csv\")\n\n# If you were unable to complete the previous lab correctly you can uncomment and load this csv\n\n# data = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DS0701EN-SkillsNetwork/api/dataset_part_2.csv')\n\ndata.head()"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>FlightNumber</th>\n      <th>PayloadMass</th>\n      <th>Flights</th>\n      <th>Block</th>\n      <th>ReusedCount</th>\n      <th>Orbit_ES-L1</th>\n      <th>Orbit_GEO</th>\n      <th>Orbit_GTO</th>\n      <th>Orbit_HEO</th>\n      <th>Orbit_ISS</th>\n      <th>...</th>\n      <th>Serial_B1058</th>\n      <th>Serial_B1059</th>\n      <th>Serial_B1060</th>\n      <th>Serial_B1062</th>\n      <th>GridFins_False</th>\n      <th>GridFins_True</th>\n      <th>Reused_False</th>\n      <th>Reused_True</th>\n      <th>Legs_False</th>\n      <th>Legs_True</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>6104.959412</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.0</td>\n      <td>525.000000</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.0</td>\n      <td>677.000000</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.0</td>\n      <td>500.000000</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3170.000000</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>85</th>\n      <td>86.0</td>\n      <td>15400.000000</td>\n      <td>2.0</td>\n      <td>5.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>86</th>\n      <td>87.0</td>\n      <td>15400.000000</td>\n      <td>3.0</td>\n      <td>5.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>87</th>\n      <td>88.0</td>\n      <td>15400.000000</td>\n      <td>6.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>88</th>\n      <td>89.0</td>\n      <td>15400.000000</td>\n      <td>3.0</td>\n      <td>5.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>89</th>\n      <td>90.0</td>\n      <td>3681.000000</td>\n      <td>1.0</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>90 rows \u00d7 83 columns</p>\n</div>",
                        "text/plain": "    FlightNumber   PayloadMass  Flights  Block  ReusedCount  Orbit_ES-L1  \\\n0            1.0   6104.959412      1.0    1.0          0.0          0.0   \n1            2.0    525.000000      1.0    1.0          0.0          0.0   \n2            3.0    677.000000      1.0    1.0          0.0          0.0   \n3            4.0    500.000000      1.0    1.0          0.0          0.0   \n4            5.0   3170.000000      1.0    1.0          0.0          0.0   \n..           ...           ...      ...    ...          ...          ...   \n85          86.0  15400.000000      2.0    5.0          2.0          0.0   \n86          87.0  15400.000000      3.0    5.0          2.0          0.0   \n87          88.0  15400.000000      6.0    5.0          5.0          0.0   \n88          89.0  15400.000000      3.0    5.0          2.0          0.0   \n89          90.0   3681.000000      1.0    5.0          0.0          0.0   \n\n    Orbit_GEO  Orbit_GTO  Orbit_HEO  Orbit_ISS  ...  Serial_B1058  \\\n0         0.0        0.0        0.0        0.0  ...           0.0   \n1         0.0        0.0        0.0        0.0  ...           0.0   \n2         0.0        0.0        0.0        1.0  ...           0.0   \n3         0.0        0.0        0.0        0.0  ...           0.0   \n4         0.0        1.0        0.0        0.0  ...           0.0   \n..        ...        ...        ...        ...  ...           ...   \n85        0.0        0.0        0.0        0.0  ...           0.0   \n86        0.0        0.0        0.0        0.0  ...           1.0   \n87        0.0        0.0        0.0        0.0  ...           0.0   \n88        0.0        0.0        0.0        0.0  ...           0.0   \n89        0.0        0.0        0.0        0.0  ...           0.0   \n\n    Serial_B1059  Serial_B1060  Serial_B1062  GridFins_False  GridFins_True  \\\n0            0.0           0.0           0.0             1.0            0.0   \n1            0.0           0.0           0.0             1.0            0.0   \n2            0.0           0.0           0.0             1.0            0.0   \n3            0.0           0.0           0.0             1.0            0.0   \n4            0.0           0.0           0.0             1.0            0.0   \n..           ...           ...           ...             ...            ...   \n85           0.0           1.0           0.0             0.0            1.0   \n86           0.0           0.0           0.0             0.0            1.0   \n87           0.0           0.0           0.0             0.0            1.0   \n88           0.0           1.0           0.0             0.0            1.0   \n89           0.0           0.0           1.0             0.0            1.0   \n\n    Reused_False  Reused_True  Legs_False  Legs_True  \n0            1.0          0.0         1.0        0.0  \n1            1.0          0.0         1.0        0.0  \n2            1.0          0.0         1.0        0.0  \n3            1.0          0.0         1.0        0.0  \n4            1.0          0.0         1.0        0.0  \n..           ...          ...         ...        ...  \n85           0.0          1.0         0.0        1.0  \n86           0.0          1.0         0.0        1.0  \n87           0.0          1.0         0.0        1.0  \n88           0.0          1.0         0.0        1.0  \n89           1.0          0.0         0.0        1.0  \n\n[90 rows x 83 columns]"
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "X = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DS0321EN-SkillsNetwork/datasets/dataset_part_3.csv')\n\n# If you were unable to complete the previous lab correctly you can uncomment and load this csv\n\n# X = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DS0701EN-SkillsNetwork/api/dataset_part_3.csv')\n\nX.head(100)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## TASK  1\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Create a NumPy array from the column <code>Class</code> in <code>data</code>, by applying the method <code>to_numpy()</code>  then\nassign it  to the variable <code>Y</code>,make sure the output is a  Pandas series (only one bracket df\\['name of  column']).\n"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": "Y = data[\"Class\"].to_numpy()\ny = Y"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## TASK  2\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Standardize the data in <code>X</code> then reassign it to the variable  <code>X</code> using the transform provided below.\n"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "StandardScaler()"
                    },
                    "execution_count": 17,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# students get this \ntransform = preprocessing.StandardScaler()\ntransform.fit(X)"
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Help on StandardScaler in module sklearn.preprocessing._data object:\n\nclass StandardScaler(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n |  StandardScaler(*, copy=True, with_mean=True, with_std=True)\n |  \n |  Standardize features by removing the mean and scaling to unit variance\n |  \n |  The standard score of a sample `x` is calculated as:\n |  \n |      z = (x - u) / s\n |  \n |  where `u` is the mean of the training samples or zero if `with_mean=False`,\n |  and `s` is the standard deviation of the training samples or one if\n |  `with_std=False`.\n |  \n |  Centering and scaling happen independently on each feature by computing\n |  the relevant statistics on the samples in the training set. Mean and\n |  standard deviation are then stored to be used on later data using\n |  :meth:`transform`.\n |  \n |  Standardization of a dataset is a common requirement for many\n |  machine learning estimators: they might behave badly if the\n |  individual features do not more or less look like standard normally\n |  distributed data (e.g. Gaussian with 0 mean and unit variance).\n |  \n |  For instance many elements used in the objective function of\n |  a learning algorithm (such as the RBF kernel of Support Vector\n |  Machines or the L1 and L2 regularizers of linear models) assume that\n |  all features are centered around 0 and have variance in the same\n |  order. If a feature has a variance that is orders of magnitude larger\n |  that others, it might dominate the objective function and make the\n |  estimator unable to learn from other features correctly as expected.\n |  \n |  This scaler can also be applied to sparse CSR or CSC matrices by passing\n |  `with_mean=False` to avoid breaking the sparsity structure of the data.\n |  \n |  Read more in the :ref:`User Guide <preprocessing_scaler>`.\n |  \n |  Parameters\n |  ----------\n |  copy : boolean, optional, default True\n |      If False, try to avoid a copy and do inplace scaling instead.\n |      This is not guaranteed to always work inplace; e.g. if the data is\n |      not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n |      returned.\n |  \n |  with_mean : boolean, True by default\n |      If True, center the data before scaling.\n |      This does not work (and will raise an exception) when attempted on\n |      sparse matrices, because centering them entails building a dense\n |      matrix which in common use cases is likely to be too large to fit in\n |      memory.\n |  \n |  with_std : boolean, True by default\n |      If True, scale the data to unit variance (or equivalently,\n |      unit standard deviation).\n |  \n |  Attributes\n |  ----------\n |  scale_ : ndarray or None, shape (n_features,)\n |      Per feature relative scaling of the data. This is calculated using\n |      `np.sqrt(var_)`. Equal to ``None`` when ``with_std=False``.\n |  \n |      .. versionadded:: 0.17\n |         *scale_*\n |  \n |  mean_ : ndarray or None, shape (n_features,)\n |      The mean value for each feature in the training set.\n |      Equal to ``None`` when ``with_mean=False``.\n |  \n |  var_ : ndarray or None, shape (n_features,)\n |      The variance for each feature in the training set. Used to compute\n |      `scale_`. Equal to ``None`` when ``with_std=False``.\n |  \n |  n_samples_seen_ : int or array, shape (n_features,)\n |      The number of samples processed by the estimator for each feature.\n |      If there are not missing samples, the ``n_samples_seen`` will be an\n |      integer, otherwise it will be an array.\n |      Will be reset on new calls to fit, but increments across\n |      ``partial_fit`` calls.\n |  \n |  Examples\n |  --------\n |  >>> from sklearn.preprocessing import StandardScaler\n |  >>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n |  >>> scaler = StandardScaler()\n |  >>> print(scaler.fit(data))\n |  StandardScaler()\n |  >>> print(scaler.mean_)\n |  [0.5 0.5]\n |  >>> print(scaler.transform(data))\n |  [[-1. -1.]\n |   [-1. -1.]\n |   [ 1.  1.]\n |   [ 1.  1.]]\n |  >>> print(scaler.transform([[2, 2]]))\n |  [[3. 3.]]\n |  \n |  See also\n |  --------\n |  scale: Equivalent function without the estimator API.\n |  \n |  :class:`sklearn.decomposition.PCA`\n |      Further removes the linear correlation across features with 'whiten=True'.\n |  \n |  Notes\n |  -----\n |  NaNs are treated as missing values: disregarded in fit, and maintained in\n |  transform.\n |  \n |  We use a biased estimator for the standard deviation, equivalent to\n |  `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to\n |  affect model performance.\n |  \n |  For a comparison of the different scalers, transformers, and normalizers,\n |  see :ref:`examples/preprocessing/plot_all_scaling.py\n |  <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n |  \n |  Method resolution order:\n |      StandardScaler\n |      sklearn.base.TransformerMixin\n |      sklearn.base.BaseEstimator\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, *, copy=True, with_mean=True, with_std=True)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  fit(self, X, y=None)\n |      Compute the mean and std to be used for later scaling.\n |      \n |      Parameters\n |      ----------\n |      X : {array-like, sparse matrix}, shape [n_samples, n_features]\n |          The data used to compute the mean and standard deviation\n |          used for later scaling along the features axis.\n |      \n |      y\n |          Ignored\n |  \n |  inverse_transform(self, X, copy=None)\n |      Scale back the data to the original representation\n |      \n |      Parameters\n |      ----------\n |      X : array-like, shape [n_samples, n_features]\n |          The data used to scale along the features axis.\n |      copy : bool, optional (default: None)\n |          Copy the input X or not.\n |      \n |      Returns\n |      -------\n |      X_tr : array-like, shape [n_samples, n_features]\n |          Transformed array.\n |  \n |  partial_fit(self, X, y=None)\n |      Online computation of mean and std on X for later scaling.\n |      \n |      All of X is processed as a single batch. This is intended for cases\n |      when :meth:`fit` is not feasible due to very large number of\n |      `n_samples` or because X is read from a continuous stream.\n |      \n |      The algorithm for incremental mean and std is given in Equation 1.5a,b\n |      in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. \"Algorithms\n |      for computing the sample variance: Analysis and recommendations.\"\n |      The American Statistician 37.3 (1983): 242-247:\n |      \n |      Parameters\n |      ----------\n |      X : {array-like, sparse matrix}, shape [n_samples, n_features]\n |          The data used to compute the mean and standard deviation\n |          used for later scaling along the features axis.\n |      \n |      y : None\n |          Ignored.\n |      \n |      Returns\n |      -------\n |      self : object\n |          Transformer instance.\n |  \n |  transform(self, X, copy=None)\n |      Perform standardization by centering and scaling\n |      \n |      Parameters\n |      ----------\n |      X : array-like, shape [n_samples, n_features]\n |          The data used to scale along the features axis.\n |      copy : bool, optional (default: None)\n |          Copy the input X or not.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from sklearn.base.TransformerMixin:\n |  \n |  fit_transform(self, X, y=None, **fit_params)\n |      Fit to data, then transform it.\n |      \n |      Fits transformer to X and y with optional parameters fit_params\n |      and returns a transformed version of X.\n |      \n |      Parameters\n |      ----------\n |      X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features)\n |      \n |      y : ndarray of shape (n_samples,), default=None\n |          Target values.\n |      \n |      **fit_params : dict\n |          Additional fit parameters.\n |      \n |      Returns\n |      -------\n |      X_new : ndarray array of shape (n_samples, n_features_new)\n |          Transformed array.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from sklearn.base.TransformerMixin:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from sklearn.base.BaseEstimator:\n |  \n |  __getstate__(self)\n |  \n |  __repr__(self, N_CHAR_MAX=700)\n |      Return repr(self).\n |  \n |  __setstate__(self, state)\n |  \n |  get_params(self, deep=True)\n |      Get parameters for this estimator.\n |      \n |      Parameters\n |      ----------\n |      deep : bool, default=True\n |          If True, will return the parameters for this estimator and\n |          contained subobjects that are estimators.\n |      \n |      Returns\n |      -------\n |      params : mapping of string to any\n |          Parameter names mapped to their values.\n |  \n |  set_params(self, **params)\n |      Set the parameters of this estimator.\n |      \n |      The method works on simple estimators as well as on nested objects\n |      (such as pipelines). The latter have parameters of the form\n |      ``<component>__<parameter>`` so that it's possible to update each\n |      component of a nested object.\n |      \n |      Parameters\n |      ----------\n |      **params : dict\n |          Estimator parameters.\n |      \n |      Returns\n |      -------\n |      self : object\n |          Estimator instance.\n\n"
                }
            ],
            "source": "help(transform)"
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": "x = transform.transform(X)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We split the data into training and testing data using the  function  <code>train_test_split</code>.   The training data is divided into validation data, a second set used for training  data; then the models are trained and hyperparameters are selected using the function <code>GridSearchCV</code>.\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## TASK  3\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Use the function train_test_split to split the data X and Y into training and test data. Set the parameter test_size to  0.2 and random_state to 2. The training data and test data should be assigned to the following labels.\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<code>X_train, X_test, Y_train, Y_test</code>\n"
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=2)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "we can see we only have 18 test samples.\n"
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "(18,)"
                    },
                    "execution_count": 21,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "Y_test.shape"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## TASK  4\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Create a logistic regression object  then create a  GridSearchCV object  <code>logreg_cv</code> with cv = 10.  Fit the object to find the best parameters from the dictionary <code>parameters</code>.\n"
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": "parameters ={'C':[0.01,0.1,1],\n             'penalty':['l2'],\n             'solver':['lbfgs']}"
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "ename": "TypeError",
                    "evalue": "__init__() missing 2 required positional arguments: 'estimator' and 'param_grid'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[0;32m<ipython-input-25-dc47e3b35be0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlogreg_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
                        "\u001b[0;32m/opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'estimator' and 'param_grid'"
                    ]
                }
            ],
            "source": "parameters ={\"C\":[0.01,0.1,1],'penalty':['l2'], 'solver':['lbfgs']}# l1 lasso l2 ridge\nlr=LogisticRegression()\n\n\nlogreg_cv = GridSearchCV()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We output the <code>GridSearchCV</code> object for logistic regression. We display the best parameters using the data attribute <code>best_params\\_</code> and the accuracy on the validation data using the data attribute <code>best_score\\_</code>.\n"
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'logreg_cv' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[0;32m<ipython-input-26-71f284dd5387>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tuned hpyerparameters :(best parameters) \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogreg_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"accuracy :\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogreg_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mNameError\u001b[0m: name 'logreg_cv' is not defined"
                    ]
                }
            ],
            "source": "print(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\nprint(\"accuracy :\",logreg_cv.best_score_)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## TASK  5\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Calculate the accuracy on the test data using the method <code>score</code>:\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Lets look at the confusion matrix:\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "yhat=logreg_cv.predict(X_test)\nplot_confusion_matrix(Y_test,yhat)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Examining the confusion matrix, we see that logistic regression can distinguish between the different classes.  We see that the major problem is false positives.\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## TASK  6\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Create a support vector machine object then  create a  <code>GridSearchCV</code> object  <code>svm_cv</code> with cv - 10.  Fit the object to find the best parameters from the dictionary <code>parameters</code>.\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "parameters = {'kernel':('linear', 'rbf','poly','rbf', 'sigmoid'),\n              'C': np.logspace(-3, 3, 5),\n              'gamma':np.logspace(-3, 3, 5)}\nsvm = SVC()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "print(\"tuned hpyerparameters :(best parameters) \",svm_cv.best_params_)\nprint(\"accuracy :\",svm_cv.best_score_)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## TASK  7\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Calculate the accuracy on the test data using the method <code>score</code>:\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We can plot the confusion matrix\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "yhat=svm_cv.predict(X_test)\nplot_confusion_matrix(Y_test,yhat)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## TASK  8\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Create a decision tree classifier object then  create a  <code>GridSearchCV</code> object  <code>tree_cv</code> with cv = 10.  Fit the object to find the best parameters from the dictionary <code>parameters</code>.\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "parameters = {'criterion': ['gini', 'entropy'],\n     'splitter': ['best', 'random'],\n     'max_depth': [2*n for n in range(1,10)],\n     'max_features': ['auto', 'sqrt'],\n     'min_samples_leaf': [1, 2, 4],\n     'min_samples_split': [2, 5, 10]}\n\ntree = DecisionTreeClassifier()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "print(\"tuned hpyerparameters :(best parameters) \",tree_cv.best_params_)\nprint(\"accuracy :\",tree_cv.best_score_)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## TASK  9\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Calculate the accuracy of tree_cv on the test data using the method <code>score</code>:\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We can plot the confusion matrix\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "yhat = svm_cv.predict(X_test)\nplot_confusion_matrix(Y_test,yhat)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## TASK  10\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Create a k nearest neighbors object then  create a  <code>GridSearchCV</code> object  <code>knn_cv</code> with cv = 10.  Fit the object to find the best parameters from the dictionary <code>parameters</code>.\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "parameters = {'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n              'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n              'p': [1,2]}\n\nKNN = KNeighborsClassifier()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "print(\"tuned hpyerparameters :(best parameters) \",knn_cv.best_params_)\nprint(\"accuracy :\",knn_cv.best_score_)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## TASK  11\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Calculate the accuracy of tree_cv on the test data using the method <code>score</code>:\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We can plot the confusion matrix\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "yhat = knn_cv.predict(X_test)\nplot_confusion_matrix(Y_test,yhat)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## TASK  12\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Find the method performs best:\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Authors\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDS0321ENSkillsNetwork26802033-2021-01-01\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Change Log\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "| Date (YYYY-MM-DD) | Version | Changed By    | Change Description      |\n| ----------------- | ------- | ------------- | ----------------------- |\n| 2021-08-31        | 1.1     | Lakshmi Holla | Modified markdown       |\n| 2020-09-20        | 1.0     | Joseph        | Modified Multiple Areas |\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Copyright \u00a9 2020 IBM Corporation. All rights reserved.\n"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.8",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}